# -*- coding: utf-8 -*-
"""llm_news_articles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gyrKab86h0MWByFZ8_b3d17tkKEanFTf
"""


import key
google_api_key=key.google_api_key

import os
import streamlit as st
import pickle
import time
from langchain_google_genai import GoogleGenerativeAI
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import UnstructuredURLLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

llm=GoogleGenerativeAI(google_api_key=google_api_key,temperature=0.4,model='gemini-1.5-flash')
main_placeholder=st.empty()

st.title('News Research tool by Abul')

st.sidebar.title('Article URLS')

urls=[]
for i in range(3):
  url=st.sidebar.text_input(f'URL{i+1}')
  urls.append(url)
processed_url=st.sidebar.button('Process URLS')
file_path="faiss_store.pkl"
if processed_url:
  main_placeholder=st.text('Data loading.....')
  loader=UnstructuredURLLoader(urls=urls)
  data=loader.load()
  main_placeholder.text('Text Splitting started...')
  text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,separators=['\n\n', '\n', '.', ','],chunk_overlap=200)
  docs=text_splitter.split_documents(data)
  main_placeholder.text('Embedding Started...')
  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
  vector_index=FAISS.from_documents(docs,embeddings)
  with open(file_path,'wb') as f:
    pickle.dump(vector_index,f)

query = main_placeholder.text_input("Question: ")
if query:
  if os.path.exists(file_path):
    with open(file_path,'rb') as f:
      vectorstore=pickle.load(f)
  chain=RetrievalQAWithSourcesChain.from_llm(llm=llm,retriever=vectorstore.as_retriever())
  response=chain({'question':query},return_only_outputs=True)
  st.header('Answer')
  st.write(response['answer'])
              # Display sources, if available
  sources = response.get("sources", "")
  if sources:
      st.subheader("Sources:")
      sources_list = sources.split("\n")  # Split the sources by newline
      for source in sources_list:
          st.write(source)